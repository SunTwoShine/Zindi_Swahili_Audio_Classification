{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4daf663-07a1-4c2e-a44c-f10974ed2a78",
   "metadata": {},
   "source": [
    "# Swahili Audio Classification (Zindi Challenge)\n",
    "\n",
    "The following model is used for classifying 12 different swahili words from wav-audiofiles. The audiofiles all have different length and usually silence at the beginning and end. Some have disturbing background noises like other people talking, children playing, clicking sounds...\n",
    "\n",
    "The audiofiles are filtered in length to reduce data to relevant parts. Here, a moving minimum filter is used to filter for long loud sections of the audio files which is usually the spoken word. The file is then cropped to 1,5 seconds around this loud section.\n",
    "\n",
    "Then the audiofile is transformed to a spectrogram. For classification, a pretrained Resnet18 model from the torchvision library is used.\n",
    "\n",
    "**Contents**\n",
    "* Creating a custom data loader for the training data set with preprocessing pipeline\n",
    "* Building a pytorch model\n",
    "* Training the model\n",
    "* Loading the model on the test data and creating the submission file\n",
    "\n",
    "For me, the device was \"**cpu**\".\n",
    "\n",
    "Main Source:\n",
    "* https://github.com/musikalkemist/pytorchforaudio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca89f2-4e7c-4fd5-a01e-08234fcdad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import minimum_filter1d\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2f4f1-2e48-479c-8988-eca102de44d8",
   "metadata": {},
   "source": [
    "## Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4c040-d014-4a23-ba7b-869e313da062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "ANNOTATIONS_FILE = \"data/Train.csv\"\n",
    "AUDIO_DIR = \"data/Swahili_words\"\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = int(SAMPLE_RATE*1.5)\n",
    "\n",
    "# Training model\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "VAL_SPLIT = .2\n",
    "SHUFFLE_DATASET = True\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf514a8-8f9b-4737-8282-325f233c79d2",
   "metadata": {},
   "source": [
    "## Custom data loader for Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd91a9-b28a-422c-a563-eac4af835ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for loading and transforming the training data\n",
    "    \n",
    "    Input:\n",
    "    * annotations_file\n",
    "    * audio_dir\n",
    "    * transformation\n",
    "    * target_sample_rate\n",
    "    * num_samples\n",
    "    * device\n",
    "    \n",
    "    Returns:\n",
    "    * signal\n",
    "    * label\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        \"\"\"\n",
    "        All sounds need to have the same sampling rate.\n",
    "        resampling if the sampling rate differs from the wanted sampling rate.\n",
    "        \"\"\"\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        All sounds need to have the same amount of layers.\n",
    "        If a sound was recorded in stereo, it is mixed down to mono.\n",
    "        \"\"\"\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        As most sounds have a silence at the beginning and end and also disturbing sounds like crackling or laughs,\n",
    "        the signal is centered to the loudest part and cut to the wanted length.\n",
    "        The loudest part is found by filtering a moving minimum filter of 1/16th second.\n",
    "        This way, very short loud impulse sounds are removed.\n",
    "        \"\"\"\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            # Cut first 0.5 seconds, because of signal problems that disturb the mininum_filter1d\n",
    "            signal=signal[:,int(self.target_sample_rate/2):]\n",
    "            # Take the minimum of every 16th second moving filter. \n",
    "            # This filters out short maxima created by impulse noises that might be louder than the speaker.\n",
    "            min_filter = minimum_filter1d(abs(signal), size=int(self.target_sample_rate/16), mode='constant')\n",
    "            ind_max = min_filter[0].argmax()\n",
    "            window_range = int(self.num_samples/2)\n",
    "            if ind_max<=window_range:\n",
    "                ind_lrange=0\n",
    "            else:\n",
    "                ind_lrange=int(ind_max-window_range)\n",
    "\n",
    "            if (signal.shape[1]-ind_max)<=window_range:\n",
    "                ind_rrange=int(signal.shape[1])\n",
    "            else:\n",
    "                ind_rrange=int(ind_max+window_range)\n",
    "            signal=signal[:,ind_lrange:ind_rrange]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        \"\"\"\n",
    "        Pad the sounds to the same length.\n",
    "        \"\"\"\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        \"\"\"\n",
    "        Path of the audio files.\n",
    "        The individual file names are found in the annotaions file in column 0\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        \"\"\"\n",
    "        Numerically encoded label.\n",
    "        The target label is found in column 1 of the annotations file. \n",
    "        Column 1 is the swahili word, Column 2 is the english equivalent. \n",
    "        \"\"\"\n",
    "        labels = ['hapana',\n",
    "                  'kumi',\n",
    "                  'mbili',\n",
    "                  'moja',\n",
    "                  'nane',\n",
    "                  'ndio',\n",
    "                  'nne',\n",
    "                  'saba',\n",
    "                  'sita',\n",
    "                  'tano',\n",
    "                  'tatu',\n",
    "                  'tisa']\n",
    "        label = labels.index(self.annotations.iloc[index, 1])\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfe763-f2ba-4fed-a093-4ab27d38ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_transforms(signal):\n",
    "    \"\"\"\n",
    "    Transformations applied to the data at loading.\n",
    "    Audio files are transformed into spectrograms.\n",
    "    The values are transformed to dB scale.\n",
    "    High frequencies are cut off as these are not containing relevant information for spoken words.\n",
    "    \n",
    "    Input: sound signal (amplitude per time)\n",
    "    Returns: spectrogram (frequency per time)\n",
    "    \"\"\"\n",
    "    N_FFT = 1024\n",
    "    \n",
    "    spectrogram = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=100,\n",
    "    )\n",
    "    to_db = torchaudio.transforms.AmplitudeToDB(stype=\"amplitude\", top_db=100)\n",
    "    \n",
    "    spec = spectrogram(signal).to(device)\n",
    "    spec_db = to_db(spec[:,:int(N_FFT/4),:])\n",
    "    return spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824f743-7baa-4139-84ab-947f23c97f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if the dataset class loads the data correctly\n",
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            audio_transforms,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            device)\n",
    "print(f\"There are {len(swahili_train)} samples in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef912c2-b606-4c7a-bdb2-7ad9bac7e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example data\n",
    "signal, label = swahili_train[10]\n",
    "print(f\"example signal: {signal.shape}, label: {label}\")\n",
    "plt.imshow(signal[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681356e3-bfd9-42ba-a0a4-ed5f2a72718c",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47621921-5e44-49a7-b54c-1f1971c77fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet, pretrained\n",
    "model_resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model_resnet.conv1=nn.Conv2d(1, model_resnet.conv1.out_channels, \n",
    "                      kernel_size=model_resnet.conv1.kernel_size[0], \n",
    "                      stride=model_resnet.conv1.stride[0], \n",
    "                      padding=model_resnet.conv1.padding[0])\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_ftrs, 12) # 12 output classes for 12 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e344da-e10c-4799-bc50-9a1edf87aa7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet = model_resnet.to(device)\n",
    "summary(resnet, (1, 256, 331)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52f9b4-9317-4259-a3b1-6f5c5b8416af",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bcedf-6c4f-43b0-960f-11524091f46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(train_data, batch_size, val_split, shuffle_dataset, random_seed):\n",
    "    \"\"\"\n",
    "    Data Loader for training and validation set from the same custom Dataset for trainings data.\n",
    "    \n",
    "    Input:\n",
    "    train_data (Custom Dataset with signal and label)\n",
    "    batch_size\n",
    "    val_split (split size)\n",
    "    shuffle_dataset (bool, if dataset is shuffled for train/val-split)\n",
    "    random_seed\n",
    "    \n",
    "    Returns:\n",
    "    train_dataloader\n",
    "    val_dataloader\n",
    "    \"\"\"\n",
    "    dataset_size = len(train_data)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(val_split * dataset_size))\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    val_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                                    sampler=val_sampler)\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "def train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device):\n",
    "    for input, target in train_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        train_prediction = model(input)\n",
    "        train_loss = loss_fn(train_prediction, target)\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimiser.step()\n",
    "    \n",
    "    for input, target in val_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        val_prediction = model(input)\n",
    "        val_loss = loss_fn(val_prediction, target)\n",
    "\n",
    "    print(f\"training loss: {train_loss.item()}, validation loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44676376-4a79-467c-8b39-2f86d89a2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                               AUDIO_DIR,\n",
    "                               audio_transforms,\n",
    "                               SAMPLE_RATE,\n",
    "                               NUM_SAMPLES,\n",
    "                               device)\n",
    "    \n",
    "train_dataloader, val_dataloader = create_data_loaders(swahili_train, \n",
    "                                                       BATCH_SIZE, \n",
    "                                                       VAL_SPLIT, \n",
    "                                                       SHUFFLE_DATASET, \n",
    "                                                       RANDOM_SEED)\n",
    "\n",
    "# construct model and assign it to device\n",
    "resnet = model_resnet.to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(resnet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train model\n",
    "train(resnet, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(resnet.state_dict(), \"resnet.pth\")\n",
    "print(\"Trained model saved at resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb4fff-afab-4f79-8b54-db9b8502cf22",
   "metadata": {},
   "source": [
    "## Competition Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b7593-9722-4af5-9598-31830379c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliDataset_Testset(SwahiliDataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for test data.\n",
    "    The label is not known for the test data, therefore the Dataset for training data cannot be used.\n",
    "    This Dataset inherits the functions defined in SwahiliDataset.\n",
    "    The same transformations are applied to the signal.\n",
    "    The getitem function is adjusted to return only the signal.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e903b1-bf6d-4b94-8d75-867da2b35b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testset(model, data_loader):\n",
    "    \"\"\"\n",
    "    Predictions on the test set\n",
    "    Returns:\n",
    "    predictions with values between 0 and 1 due to added softmax layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            input = input.to(device)\n",
    "            predictions = nn.Softmax(dim=1)(model(input))\n",
    "        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n",
    "    return predictions\n",
    "\n",
    "def create_test_data_loader(test_data):\n",
    "    \"\"\"\n",
    "    test data loader with full test data set as batch\n",
    "    \"\"\"\n",
    "    dataset_size = len(test_data)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=dataset_size)\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf83ca-5d97-4807-8284-af90a68a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the model\n",
    "resnet = model_resnet.to(device)\n",
    "state_dict = torch.load(\"resnet.pth\")\n",
    "resnet.load_state_dict(state_dict)\n",
    "\n",
    "TEST_ANNOTATIONS_FILE = \"data/Test.csv\"\n",
    "\n",
    "swahili_test = SwahiliDataset_Testset(TEST_ANNOTATIONS_FILE,\n",
    "                                      AUDIO_DIR,\n",
    "                                      audio_transforms,\n",
    "                                      SAMPLE_RATE,\n",
    "                                      NUM_SAMPLES,\n",
    "                                      \"cpu\")\n",
    "\n",
    "test_dataloader = create_test_data_loader(swahili_test)\n",
    "\n",
    "predicted = predict_testset(resnet, test_dataloader)\n",
    "print(f\"Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f1a07-45db-4d7f-b582-865edf7a1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2233fb-20c5-47b2-b19f-30821ace8fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "class_mapping = ['hapana',\n",
    "                  'kumi',\n",
    "                  'mbili',\n",
    "                  'moja',\n",
    "                  'nane',\n",
    "                  'ndio',\n",
    "                  'nne',\n",
    "                  'saba',\n",
    "                  'sita',\n",
    "                  'tano',\n",
    "                  'tatu',\n",
    "                  'tisa']\n",
    "\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "submission = pd.DataFrame({'Word_id': test['Word_id']})\n",
    "for i, label in enumerate(class_mapping):\n",
    "    submission[label] = predicted[:,i].numpy()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e0bdf-79c4-4afb-819c-690c3e5f490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('data/submission_torch_resnet18_normal-spec_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfa3e4-1f18-4879-a5c0-231fdb59a8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
