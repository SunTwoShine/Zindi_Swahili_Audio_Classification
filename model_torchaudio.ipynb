{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4daf663-07a1-4c2e-a44c-f10974ed2a78",
   "metadata": {},
   "source": [
    "Orientation on:\n",
    "https://github.com/musikalkemist/pytorchforaudio/tree/main/10%20Predictions%20with%20sound%20classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca89f2-4e7c-4fd5-a01e-08234fcdad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.ndimage import minimum_filter1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf514a8-8f9b-4737-8282-325f233c79d2",
   "metadata": {},
   "source": [
    "## Custom data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd91a9-b28a-422c-a563-eac4af835ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            # Cut first 0.5 seconds, because of signal problems that disturb the mininum_filter1d\n",
    "            signal=signal[:,int(self.target_sample_rate/2):]\n",
    "            # Take the minimum of every 16th second moving filter. \n",
    "            # This filters out short maxima created by impulse noises that might be louder than the speaker.\n",
    "            min_filter = minimum_filter1d(abs(signal), size=int(self.target_sample_rate/16), mode='constant')\n",
    "            ind_max = min_filter[0].argmax()\n",
    "            window_range = int(self.num_samples/2)\n",
    "            if ind_max<=window_range:\n",
    "                ind_lrange=0\n",
    "            else:\n",
    "                ind_lrange=int(ind_max-window_range)\n",
    "\n",
    "            if (signal.shape[1]-ind_max)<=window_range:\n",
    "                ind_rrange=int(signal.shape[1])\n",
    "            else:\n",
    "                ind_rrange=int(ind_max+window_range)\n",
    "            signal=signal[:,ind_lrange:ind_rrange]\n",
    "            # signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        labels = ['hapana',\n",
    "                  'kumi',\n",
    "                  'mbili',\n",
    "                  'moja',\n",
    "                  'nane',\n",
    "                  'ndio',\n",
    "                  'nne',\n",
    "                  'saba',\n",
    "                  'sita',\n",
    "                  'tano',\n",
    "                  'tatu',\n",
    "                  'tisa']\n",
    "        label = labels.index(self.annotations.iloc[index, 1])\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824f743-7baa-4139-84ab-947f23c97f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ANNOTATIONS_FILE = \"data/Train.csv\"\n",
    "    AUDIO_DIR = \"data/Swahili_words\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    NUM_SAMPLES = 22050*3\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "    \n",
    "    swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            mel_spectrogram,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            device)\n",
    "    print(f\"There are {len(swahili_train)} samples in the dataset.\")\n",
    "    signal, label = swahili_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3c54a-b847-449c-a262-bb62f4a2548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa95ee5-5d96-4fe8-90d5-04efb2021423",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681356e3-bfd9-42ba-a0a4-ed5f2a72718c",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91614a89-dadf-47f9-b596-2a2eee1338d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 10, 12)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522e19e-d00a-4d5b-9a87-dca97670ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cnn = CNNNetwork()\n",
    "    summary(cnn, (1, 64, 130))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52f9b4-9317-4259-a3b1-6f5c5b8416af",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bcedf-6c4f-43b0-960f-11524091f46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "ANNOTATIONS_FILE = \"data/Train.csv\"\n",
    "AUDIO_DIR = \"data/Swahili_words\"\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050*3\n",
    "\n",
    "\n",
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    # instantiating our dataset object and create data loader\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "    \n",
    "    swahili_train = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            mel_spectrogram,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            device)\n",
    "    \n",
    "    train_dataloader = create_data_loader(swahili_train, BATCH_SIZE)\n",
    "\n",
    "    # construct model and assign it to device\n",
    "    cnn = CNNNetwork().to(device)\n",
    "    print(cnn)\n",
    "\n",
    "    # initialise loss funtion + optimiser\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.Adam(cnn.parameters(),\n",
    "                                 lr=LEARNING_RATE)\n",
    "\n",
    "    # train model\n",
    "    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
    "\n",
    "    # save model\n",
    "    torch.save(cnn.state_dict(), \"cnnnet.pth\")\n",
    "    print(\"Trained cnn net saved at cnnnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fd398-e3ac-4997-8e4b-8e2033d20256",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da36b48-0cbf-4edc-bbf1-92c3d3da50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = ['hapana',\n",
    "                  'kumi',\n",
    "                  'mbili',\n",
    "                  'moja',\n",
    "                  'nane',\n",
    "                  'ndio',\n",
    "                  'nne',\n",
    "                  'saba',\n",
    "                  'sita',\n",
    "                  'tano',\n",
    "                  'tatu',\n",
    "                  'tisa']\n",
    "\n",
    "\n",
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n",
    "        predicted_index = predictions[0].argmax(0)\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "    return predicted, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e3856-ef8b-4515-ad5b-f06a29316062",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load back the model\n",
    "    cnn = CNNNetwork()\n",
    "    state_dict = torch.load(\"cnnnet.pth\")\n",
    "    cnn.load_state_dict(state_dict)\n",
    "\n",
    "    # load urban sound dataset dataset\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "    \n",
    "    swahili_val = SwahiliDataset(ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            mel_spectrogram,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            \"cpu\")\n",
    "\n",
    "    # get a sample from the swahili dataset for inference\n",
    "    input, target = swahili_val[0][0], swahili_val[0][1] # [batch size, num_channels, fr, time]\n",
    "    input.unsqueeze_(0)\n",
    "\n",
    "    # make an inference\n",
    "    predicted, expected = predict(cnn, input, target,\n",
    "                                  class_mapping)\n",
    "    print(f\"Predicted: '{predicted}', expected: '{expected}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb4fff-afab-4f79-8b54-db9b8502cf22",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fb198-0092-4a95-a206-6409fe211e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliDataset_Testset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            # Cut first 0.5 seconds, because of signal problems that disturb the mininum_filter1d\n",
    "            signal=signal[:,int(self.target_sample_rate/2):]\n",
    "            # Take the minimum of every 16th second moving filter. \n",
    "            # This filters out short maxima created by impulse noises that might be louder than the speaker.\n",
    "            min_filter = minimum_filter1d(abs(signal), size=int(self.target_sample_rate/16), mode='constant')\n",
    "            ind_max = min_filter[0].argmax()\n",
    "            window_range = int(self.num_samples/2)\n",
    "            if ind_max<=window_range:\n",
    "                ind_lrange=0\n",
    "            else:\n",
    "                ind_lrange=int(ind_max-window_range)\n",
    "\n",
    "            if (signal.shape[1]-ind_max)<=window_range:\n",
    "                ind_rrange=int(signal.shape[1])\n",
    "            else:\n",
    "                ind_rrange=int(ind_max+window_range)\n",
    "            signal=signal[:,ind_lrange:ind_rrange]\n",
    "            # signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e903b1-bf6d-4b94-8d75-867da2b35b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testset(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input in data_loader:\n",
    "            input = input.to(device)\n",
    "            predictions = model(input)\n",
    "        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n",
    "    return predictions\n",
    "\n",
    "# def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "#     for input, target in data_loader:\n",
    "#         input, target = input.to(device), target.to(device)\n",
    "        \n",
    "#         # calculate loss\n",
    "#         prediction = model(input)\n",
    "#         loss = loss_fn(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf83ca-5d97-4807-8284-af90a68a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load back the model\n",
    "    cnn = CNNNetwork()\n",
    "    state_dict = torch.load(\"cnnnet.pth\")\n",
    "    cnn.load_state_dict(state_dict)\n",
    "\n",
    "    # load urban sound dataset dataset\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "    \n",
    "    TEST_ANNOTATIONS_FILE = \"data/Test.csv\"\n",
    "    \n",
    "    swahili_test = SwahiliDataset_Testset(TEST_ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            mel_spectrogram,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            \"cpu\")\n",
    "    \n",
    "    BATCH_SIZE = 1800\n",
    "\n",
    "    test_dataloader = create_data_loader(swahili_test, BATCH_SIZE)\n",
    "    \n",
    "    # get a sample from the swahili dataset for inference\n",
    "    # input = swahili_test[0] # [batch size, num_channels, fr, time]\n",
    "    # input.unsqueeze_(0)\n",
    "\n",
    "    # make an inference\n",
    "    predicted = predict_testset(cnn, test_dataloader)\n",
    "    print(f\"Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f1a07-45db-4d7f-b582-865edf7a1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2233fb-20c5-47b2-b19f-30821ace8fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/Test.csv')\n",
    "submission = pd.DataFrame({'Word_id': test['Word_id']})\n",
    "for i, label in enumerate(class_mapping):\n",
    "    submission[label] = predicted[:,i].numpy()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e0bdf-79c4-4afb-819c-690c3e5f490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('data/submission_torchaudio.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786a54e-9add-4cc1-9023-487f1ddca24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
